---
title: "Text Analysis in R"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
tutorial:
  id: "text"
  version: 0.5
---
```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(dplyr)
knitr::opts_chunk$set(exercise.checker = gradethis::grade_learnr)
knitr::opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
	             collapse=TRUE)
learnr::tutorial_options(
  exercise.timelimit = 60)
```

## Text mining with R
In this tutorial, we will learn about how to work with text data in R. We will learn how to turn documents into word lists, analyze frequency counts, extract bigrams, analyze sentiment and parts of speech, and how to visualize text analyses.

Before we begin, here are some useful links:

* [Text Mining with R by Julia Silge and David Robinson](https://www.tidytextmining.com/index.html)
* [Wordclouds on R Graph Gallery](https://www.r-graph-gallery.com/wordcloud.html)

To get started, we will analyze the classic Malinowski (1922) text [*Argonauts of the Western Pacific*](http://www.gutenberg.org/ebooks/55822). The text can be downloaded from Project Gutenberg, or for simplicity, we can download the text directly using the `gutenbergr` package.

First, we need to load all of the libraries we will be using today. Since this environment is more streamlined, we don't need to install them here separately. But be careful to install and load them while working on your own computers.

```{r message=FALSE, warning=FALSE}
#install.packages("gutenbergr")
library(gutenbergr)
#gutenberg_metadata
library(tidyverse)
library(wordcloud)
library(tidytext)
library(stringr)
library(topicmodels)
library(data.table)
library(textdata)
library(cleanNLP)
cnlp_init_udpipe()
```

Download the Malinowski book text and examine the structure. How are the data organized?

```{r}
malinowski1922 <- gutenberg_download(55822)
str(malinowski1922)
```

## Analyzing individual word frequencies
One of the first ways we can explore a text is through looking at word frequncies. With multiple samples from different people or sites, comparing word frequencies can reveal differences across populations, while within a single text, word frequencies can highlight key issues, people, or places in a text.

### Try it
Using the `unnest_tokens()` function, extract out the individual words from Malinowski and create a table sorting the top words by count. What do you notice about the top words? Why do you think these words appear at the top of the list?

```{r exercise1, exercise = TRUE}

```

```{r exercise1-check}
grade_result(
  pass_if(~ identical(.result, malinowski1922 %>% unnest_tokens(output=word,input=text) %>% count(word,sort=T) %>% top_n(50)), "Good Job!"),
  fail_if(~ TRUE)
)
```


## Stop words 
Many of these top words are what we call *stop words*, or those that add little to our analysis. These include words like *is*, *the* and *so*, that add little to our understanding of the overall topics or themes in a text. `Tidytext` has a built in dictionary of stop words, making it easy to quickly remove these words from the text.

```{r}
# look at the words in the stop_words dataset
data("stop_words")
stop_words %>% top_n(50)
#remove stop words from the text
words <- as.data.frame.character(words)
colnames(words) <- "word"
malinowski1922tidy <- words %>% anti_join(stop_words, by = c("word"))
#look at the structure 
str(malinowski1922tidy)
```

Now we can look at the number of unique words and their counts in Malinowski, without interference from stop words.

```{r}
#how many unique words are there?
length(unique(malinowski1922tidy$word))
#make a table of the top words with stop words removed
malinowski1922tidy_wordcounts <- malinowski1922tidy %>% count(word, sort=T) 

##look at top 50 words
malinowski1922tidy %>% count(word,sort=TRUE) %>% top_n(50) %>% mutate(word=reorder(word,n)) %>% data.frame()
```

Make a plot of these top words. What do you make of these new top words?

```{r}
#plot top words from tokenized tweets
top50wordsplot <- malinowski1922tidy %>% count(word,sort=TRUE) %>% top_n(50) %>% mutate(word=reorder(word,n))%>% ggplot(aes(x=word,y=n))+ geom_col()+xlab(NULL)+coord_flip()+labs(y="Count",x="Unique words", title="Malinowski 1922")
top50wordsplot
```

## Wordclouds

Wordclouds are often avoided in scientific research due to their sometimes misleading arrangements and sizes of words. This can make them difficult to interpret. At the same time, word clouds can be useful in exploratory data analysis or applied research for quickly showing the main themes in a text, that can then be explored for further contextual information. Here we will make wordclouds of Malinowski's text using two different methods. 

```{r}
malinowski1922tidy %>% count(word) %>%  with(wordcloud(word, n, max.words = 100))
```

Another way to make wordclouds using the [WordCloud2](https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html) package. 

```{r}
#install the package
#require(devtools)
#install_github("lchiffon/wordcloud2")
#load package
library(wordcloud2)
#make wordcloud. you may want to expand out the figure for the full effect while running separately
# wordcloud2(data = malinowski1922tidy_wordcounts)
```

## Analyzing pairs of words
We can also analyze pairs of words (bigrams). This can be useful for understanding the context around particular words as well as for identifying themes that are made up of multiple strings (e.g. "climate change", "public health").

```{r}
bigrams<- malinowski1922 %>% unnest_tokens(output=bigrams,input=text, token="ngrams",n=2)
str(bigrams)
##look at counts for each pair
bigrams %>% count(bigrams, sort = TRUE) %>% top_n(20)
```

One challenge here is that again the stop words rise to the top of the frequencies. There are multiple ways we can handle this, but here we will remove any bigrams whre either the first or second word is a stop word. 

```{r}
#seperate words to pull out stop words
separated_words <- bigrams %>% separate(bigrams, c("word1", "word2"), sep = " ")
#filter out stop words
malinowski_bigrams <- separated_words %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
```

### Try it

Try out these questions to test your own understanding!

1. Make a table of the top 100 bigrams sorted from most to least frequent. 

```{r exercise2, exercise = TRUE}

```

```{r exercise2-check}
grade_result(
  pass_if(~ identical(.result, malinowski_bigrams %>% count(word1, word2, sort = TRUE)
malinowski_bigrams_count %>% top_n(20) %>% head(100) %>% data.frame()), "Good Job!"),
  fail_if(~ TRUE)
)
```

2. Pull out all bigrams where "island" is the second term and make a table of the most common bigrams in this subset.

```{r exercise3, exercise = TRUE}

```

```{r exercise3-check}
grade_result(
  pass_if(~ identical(.result, malinowski_bigrams %>% filter(word2 == "island") %>% count(word1, word2, sort = TRUE) %>% top_n(20)), "Good Job!"),
  fail_if(~ TRUE)
)
```

3. Pull out all bigrams where "canoe" is either the first or second term and make a table of the most common bigrams in this subset.

```{r exercise4, exercise = TRUE}

```

```{r exercise4-check}
grade_result(
  pass_if(~ identical(.result, malinowski_bigrams %>% filter(word1 == "canoe" | word2 == "canoe") %>% count(word1, word2, sort = TRUE) %>% top_n(20)), "Good Job!"),
  fail_if(~ TRUE)
)
```

Before moving forward, try to think what this analysis tells you about this text? Can you think of any data in your own research that would benefit from ngram analysis?

## Sentiment analysis

Texts often contain certain emotions, feelings, or sentiments that can tell us more about what they mean. In a way, coding text data for sentiments is similar to the qualitative reseach method of coding fieldnotes for themes. Because of this, you can develop your own custom lexicon for your research context. However, because this is a popular methodology, many existing sentiment analysis dictionaries have been developed and publicly shared.

We'll work with the [NRC Emotion Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). First, we can load the NRC lexicon and look at the different types of sentiments that it contains. 

```{r}
# load the nrc sentiment dictionary
#nrcdf <- get_sentiments("nrc")
#take a look at the top sentiments that occur in the lexicon
#nrcdf %>% count(sentiment,sort=T)
```

Using `inner_join()` we can combine the sentiments with the words from Malinowski, effectively "tagging" each word with a particular sentiment.

```{r}
#merge sentiments to malinowski data
malinowski1922_sentiment <- malinowski1922tidy %>% inner_join(get_sentiments("nrc"))
```

### Try it

With the new merged and tagged dataframe, make a table of the top words in Malinowski that are associated with the sentiment "trust" and one other sentiment of choice. Reflect on how you might interpret these results. Do you find this information useful? Is there any place you could see sentiment analysis being useful in your own research?

```{r exercise5, exercise=TRUE}

```

```{r exercise5-check}
grade_result(
  pass_if(~ identical(.result, malinowski1922_sentiment %>% filter(sentiment=="trust") %>% count(word,sort=T)), "Good Job!"),
  fail_if(~ TRUE)
)
```

```{r sentiment_analysis_practice, exercise = TRUE, exercise.cap = "sentiment"}

```

## Case study: Permafrost and climate change survey
Now that we've learned a bit about text analysis using Malinowski let's test our skills on a real world dataset. Here we will use data from a survey in two Inupiaq villages in Alaska to examine how indiviuals in these communities feel about climate change and thawing permafrost. These data are drawn from here: William B. Bowden 2013. *Perceptions and implications of thawing permafrost and climate change in two Inupiaq villages of arctic Alaska* [Link](https://arcticdata.io/catalog/view/doi%3A10.18739%2FA23Z48). Let's further examine the responses to two open ended questions: **(Q5) What is causing it [permafrost around X village] to change?** and **(Q69) "What feelings do you have when thinking about the possibility of future climate change in and around [village name]?"**.

First we load the data and subset out the columns of interest. 

```{r}
#we will work with the permafrost survey data.
surv<- read.csv("https://maddiebrown.github.io/ANTH630/data/Survey_AKP-SEL.csv", stringsAsFactors = F)
surv_subset <- surv %>% select(Village, Survey.Respondent, Age.Group, X69..Feelings, X5..PF.Cause.) 
```

Then we can quickly calculate the most frequent terms across all 80 responses.

```{r}
class(surv$X69..Feelings) #make sure your column is a character variable
surv_tidy <- surv_subset %>% unnest_tokens(word, X69..Feelings) %>% anti_join(stop_words)
#what are most common words?
feelingswordcount <- surv_tidy %>% count(word,sort=T)
```

### Try it

Make wordclouds of the word frequency in responses about feelings related to climate change using two different methods.

```{r exercise6, exercise = TRUE}

```

```{r exercise6-solution}
surv_tidy %>% count(word) %>%  with(wordcloud(word, n, max.words = 100))
wordcloud2(data = feelingswordcount)
```

## Comparing word frequency across samples
Are there noticeable differences in responses across individuals from different sites? We can compare the responses about "What feelings do you have when thinking about the possibility of future climate change in and around [village name]?" from the permafrost survey, based on which village the respondent lives in.

```{r}
#word frequency by village
surv_tidy <- surv_subset %>% unnest_tokens(word, X69..Feelings) %>% anti_join(stop_words)

#what are most common words?
surv_tidy %>% count(word,sort=T) %>% top_n(20)

#we can also look at the top words
byvillage <- surv_tidy %>% count(Village,word,sort=T) %>% ungroup()
byvillage %>% top_n(20)

top_10 <- byvillage %>%
  group_by(Village) %>%
  top_n(10, n) %>%
  ungroup() %>%
  arrange(Village, desc(n))

ggplot(top_10, aes(x=reorder(word,n),y=n)) + geom_bar(stat="identity") +coord_flip() +ggtitle("Top terms by village") + labs(x="Word", y="Count") +facet_wrap(~ Village, scales = "free_y") 

```

## Topic Modeling

In addition to analyzing word and bigram frequencies, we can also analyze texts using topic modeling. Topic modeling allows us to identify themes in the text without needing to clearly know which themes or groupings we expect to emerge. This can be very useful when you have large columes of messy data or data from multiple diverse sources that you need to parse. We will use Latent Dirichlet allocation or (LDA), following the explanation in [Text Mining with R](https://www.tidytextmining.com/topicmodeling.html). 

Before we can identify themes across responses however, we need to make sure each "document" or "response" has a unique identifier. 

### Try it

*Think about it!*
What is the primary key or unique identifier for this dataset? How do you know? Why can't you use Survey.Respondent as a unique identifier?

Make a new primary key called "ID" that has a different value for each unique response.

```{r exercise7, exercise = TRUE}


```

```{r exercise7-solution}
surv_subset %>% select(Village, Survey.Respondent)
surv_subset<- surv_subset %>% mutate(ID=paste(Village,Survey.Respondent, sep="_"))
```

```{r exercise7-check}
grade_code()
```

### Frequency of word pairs per response

In this section we will look at frequency of different word pairs/bigrams per response, in our dataset. The additional comments with the code are intented to help you understand the functions of the syntax we are using.

```{r}
#look at the bigrams in these responses.
surv_subset %>% unnest_tokens(output=bigrams,input=X69..Feelings, token="ngrams",n=2) %>% count(bigrams,sort=T) %>% top_n(20)

#look at pairwise counts per responses. How often do two words show up together in one person's response?
library(widyr)
surv_tidy %>% pairwise_count(word, Survey.Respondent,sort=T)
# so we see that "change" and "cold" appear in three responses, as do climate and change. however, we previously learned that the Survey.Respondent column is not a unique identifier for the responses. Let's run the same code, but with the new ID column we created.

###let's make a new surv_tidy object that incorporates the new ID we made
surv_subset<- surv_subset %>% mutate(ID=paste(Village,Survey.Respondent, sep="_"))
surv_tidy <- surv_subset %>%  unnest_tokens(word, X69..Feelings) %>% anti_join(stop_words)
surv_tidy %>% pairwise_count(word, ID, sort=T) %>% top_n(20)
#in this case the output is nearly the same, but in other cases this distinction can make a significant difference.
```

The first step in creating a topic model is to count the number of times each word appears in each individual document (or response in our case). Luckily, we can count by two variables using the `count()` function. Let's create a new `byresponse` variable. 

```{r}
byresponse <- surv_tidy %>% count(ID,word,sort=T) %>% ungroup()

#check how many responses are included in the analysis. this allows you to double check that the new unique identifier we made worked as expected.
unique(byresponse$ID)
length(unique(byresponse$ID))
```

Now we can convert our longform word list into a document-term matrix. Read more [here](https://www.tidytextmining.com/dtm.html#cast-dtm)

```{r}
surv_dtm <- byresponse %>% cast_dtm(ID, word, n)
# ?cast_dtm #read up on how this function works
```

Run the LDA() function and choose a number of solutions. In this case, let's try it with 2.

```{r}
surv_lda <- LDA(surv_dtm, k = 2, control = list(seed = 9999))
#look at our output
str(surv_lda)

#examine the probability that each word is in a particular topic group
surv_topics <- tidy(surv_lda, matrix = "beta")
surv_topics
```

Examine the top words for each topic identified by the model.

```{r}
top_words <- surv_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta))
top_words
```

We can also examine the results graphically.

```{r}
#plot these top words for each topic (adapted from https://www.tidytextmining.com/topicmodeling.html)
top_words %>% group_by(topic) %>%
  mutate(term = fct_reorder(term, beta)) %>% ungroup() %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + theme_minimal()
```

### Try it
Repeat the topic modeling analysis but using 6 topics instead of two. Try by yourself first.

```{r TM_with_6topics, exercise = TRUE, exercise.cap = "TopicModeling"}








```

<div id="TM_with_6topics-hint">
**Hint :**What change should be there in the previous code when the number of topics is changing from 2 to 4? Think about the parameter we used to denote the number of topics inside LDA().
</div>

In this case, our sample is small, so topic modeling is not necessarily the best method to use. However, even from this small sample, you can see that some topics emerge from the text that were not previously apparent.  

## Manual text wrangling

Sometimes you'll need to edit text or strings manually. For example, you may find that for your research question, you are less interested in differentiating between the terms *running*, *run*, and *runner*, than in identifying clusters of beliefs about running as a more general concept. On the other hand, you might want to differentiate between *runners* and *running* as beliefs about groups of people vs. the act of running. How you choose to transform text data in your research is up to your research questions and understanding of the cultural context.

R has a number of helpful functions for manually adjusting strings. We'll cover a few to get you started. Let's go back to the permafrost and climate change survey and look at responses to: **(Q5) What is causing it [permafrost around X village] to change?**. 

First let's look at the raw data. What are some potential issues in the strings below that might make text analysis difficult or ineffective?

```{r}
surv$X5..PF.Cause.[10:30]
```

Luckily we can manually adjust the strings to make them easier to analyze systematically. For example we might set characters to lowercase, trim whitespace and remove any empty or missing rows.

```{r}
#make a new column to hold the tidy data
surv$cause_tidy <- surv$X5..PF.Cause.
#make lower case
surv$cause_tidy <- tolower(surv$cause_tidy)
#remove white space at beginning and end of string
surv$cause_tidy<- trimws(surv$cause_tidy)
#filter out blank or empty rows
surv<- surv %>% filter(surv$cause_tidy!="") 
surv <- surv %>% filter(surv$cause_tidy!="(blank)")
surv <- surv %>% filter(surv$cause_tidy!="n/a")
```

We can also directly replace particular strings. Here we change some strings with typos.

```{r}
surv$cause_tidy<- surv$cause_tidy %>% str_replace("wamer", "warmer")
surv$cause_tidy<- surv$cause_tidy %>% str_replace("lnoger", "longer")
```

Another common string data transformation involves grouping together responses into more standardized categories. You can transform cll values individually or based on exact string matches. In addition, using `%like%` we can transform any strings where just part of the string contains a particular string. For example, we might decide that any time the string "warm" appears in a response, the overall theme of the response is associated with "global warming". Or based on our ethnographic understanding of the context we might know that "seasonal changes" are important causes of permafrost in local cultural models. We can then look for some key terms that will allow us to rapidly change multiple responses that are likely to fit in this category. In this case, "late" and "early".

```{r}
#group some responses together based on the presence of a particular string
surv <- surv %>% mutate(cause_tidy=replace(cause_tidy,cause_tidy %like% "warm","global warming")) 
surv$cause_tidy[1:30]
surv <- surv %>% mutate(cause_tidy=replace(cause_tidy,cause_tidy %like% "early"|cause_tidy %like% "late","seasonal changes")) 
surv$cause_tidy[1:30]

#compare the original with your categorizations
#surv %>% select(X5..PF.Cause.,cause_tidy)
```

We won't get into too much detail today, but you can also search and select string data using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). You can read more in [R4DS](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions). Here let's use `str_detect()` to pull out some strings with regular expressions.

```{r}

#any responses ending in "ing"
surv$cause_tidy[str_detect(surv$cause_tidy,"ing$")]

#any reponses that contain a W followed by either an 'e' or an 'a'
surv$cause_tidy[str_detect(surv$cause_tidy,"w[ea]")]

#any responses that contain the string erosion 
surv$cause_tidy[str_detect(surv$cause_tidy,"erosion")]
# any responses that contain the string erosion, but which have any character occurring before the word erosion.
surv$cause_tidy[str_detect(surv$cause_tidy,".erosion")]

```
The utility of regular expressions is huge for quickly searching through and transforming large volumes of string data. We've only scratched the surface today.

Whenever transforming large volumes of data using string detection and regular expressions it is critical to double check that each operation is in fact working as you expected it to. Paying attention to the order of transformations is also important for preventing you from overwriting previous data transformations. 

### Creating new variables with `str_detect()`
Sometimes it is useful to create flags or indicator variables in your data. These can allow you to quickly filter out rows that have particular characteristics. For example, we can create a new binary column that indicates whether or not the response refers to global warming. This variable can then be used for further grouping, data visualization or other tasks.

```{r}
surv <- surv %>% mutate(GlobalWarmingYN=str_detect(cause_tidy,"global warming"))
table(surv$GlobalWarmingYN) # how many responses contain the string global warming?
```

### Parts of speech tagging

We can also tag the parts of speech in a text. This allows us to focus an analysis on verbs, nouns, or other parts of speech that may be of interest. For example, in a study on sentiments, we might want to pull out adjectives in order to understand how people feel or describe a particular phenomenon. On the other hand, we might also pull out verbs in order to understand the types of actions people describe as associated with certain cultural practices or beliefs. Let's tag the parts of speech in Malinowski 1922 to learn more about the places and cultural practices documented in this book.

### Try it

Using the `cnlp_annotate()` function we can tag the parts of speech in Malinowski 1922. This function can take a long time to run. This is the last thing we will do today, so feel free to let it run and then take a break and come back to finish these problems. Try by yourself first. Look for solutions only if needed. 

1. Make a new object using only the token part of the output from `cnlp_annotate()` and then examine the `$upos` column. What are all the unique parts of speech in this dataset?

2. Select and examine the top 30 nouns and verbs in this dataset. Do any of the terms surprise you? How might this level of analysis of the text be meaningful for your interpretation of its themes?

```{r exercise8, exercise = TRUE, exercise.cap = "parts_of_speech"}




```

<details>
<summary><span style="color:tomato">**Click for solution**</span></summary>
```{r results='hide'}
library(cleanNLP)
cnlp_init_udpipe()
#tag parts of speech. takes a long time
malinowksiannotatedtext <- cnlp_annotate(malinowski1922tidy$word)
str(malinowksiannotatedtext) # look at the structure. because it is a list we have to pull out that particular section of the list
malinowskiannotatedtextfull <-data.frame(malinowksiannotatedtext$token)
str(malinowskiannotatedtextfull)

# what are all the different parts of speech that have been tagged?
unique(malinowskiannotatedtextfull$upos)

#verb analysis. first look at some of the verbs that occur in the book
#malinowskiannotatedtextfull %>% filter(upos=="VERB") %>% select(token,lemma) %>% data.frame() %>% top_n(30)
#top 50 verbs
malinowskiannotatedtextfull %>% filter(upos=="VERB") %>% count(token, sort=T) %>% top_n(30)

#what are the top 50 nouns?
malinowskiannotatedtextfull %>% filter(upos=="NOUN") %>% count(lemma, sort=T) %>% top_n(30) %>% data.frame()
```
</details>



